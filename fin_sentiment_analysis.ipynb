{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuuUpI19evI9"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Import pandas for data manipulation\n",
        "import pandas as pd\n",
        "\n",
        "# --- Load the dataset ---\n",
        "# **IMPORTANT**: Replace 'financial_sentiment.csv' with the actual filename\n",
        "#              if you uploaded a file with a different name.\n",
        "try:\n",
        "    # Try loading with default comma separator\n",
        "    df = pd.read_csv('/content/financial_sentiment.csv', encoding='ISO-8859-1') # Added encoding for potential issues\n",
        "    print(\"CSV loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: CSV file not found. Make sure the file name is correct and it's uploaded.\")\n",
        "    # You might need to add code here to mount Google Drive if the file is there,\n",
        "    # or provide an alternative path.\n",
        "    df = None\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "    # If the first attempt fails, try specifying the separator if it's not a comma,\n",
        "    # or try a different encoding like 'utf-8'\n",
        "    # Example: df = pd.read_csv('your_file.csv', sep=';', encoding='utf-8')\n",
        "    df = None\n",
        "\n",
        "\n",
        "# --- Basic Exploration (only if df was loaded successfully) ---\n",
        "if df is not None:\n",
        "    print(\"\\n--- First 5 Rows ---\")\n",
        "    print(df.head())\n",
        "\n",
        "    print(\"\\n--- Dataset Info ---\")\n",
        "    df.info()\n",
        "\n",
        "    # Let's rename columns for clarity if needed (adjust based on actual names)\n",
        "    # Example: df.rename(columns={'Sentence': 'text', 'Sentiment': 'label'}, inplace=True)\n",
        "    # Check the actual column names from df.info() or df.head() first!\n",
        "    # For now, let's assume the names are 'Sentence' and 'Sentiment' as you mentioned.\n",
        "\n",
        "    print(\"\\n--- Sentiment Value Counts ---\")\n",
        "    # Check the unique values and their counts in the 'Sentiment' column\n",
        "    print(df['Sentiment'].value_counts())\n",
        "\n",
        "    print(\"\\n--- Check for Missing Values ---\")\n",
        "    print(df.isnull().sum())\n",
        "\n",
        "    # Clean up potential extra columns if 'Index' is just a row number\n",
        "    # if 'Index' in df.columns:\n",
        "    #     df = df[['Sentence', 'Sentiment']] # Keep only relevant columns\n",
        "    #     print(\"\\nKept only 'Sentence' and 'Sentiment' columns.\")\n",
        "\n",
        "    # Store the assumed column names for later use\n",
        "    text_column = 'Sentence'\n",
        "    label_column = 'Sentiment'\n",
        "\n",
        "    print(f\"\\nUsing '{text_column}' as the text column.\")\n",
        "    print(f\"Using '{label_column}' as the label column.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "O5JX5a58flAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Sentiment'].value_counts()"
      ],
      "metadata": {
        "id": "DcrFANEbfqHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries from Hugging Face\n",
        "!pip install datasets transformers[torch] accelerate -U\n",
        "\n",
        "# --- Now the rest of your imports and code ---\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from datasets import Dataset, DatasetDict # Hugging Face datasets library\n",
        "import numpy as np\n",
        "import pandas as pd # Make sure pandas is imported if not already\n",
        "\n",
        "# --- Make sure df is loaded and has the correct columns ---\n",
        "if 'df' not in locals() or df is None:\n",
        "    print(\"Error: DataFrame 'df' not found or is None. Please load the data first.\")\n",
        "    # Add fallback or stop execution if df isn't loaded\n",
        "else:\n",
        "    text_column = 'Sentence' # Make sure this matches your actual column name\n",
        "    label_column = 'Sentiment' # Make sure this matches your actual column name\n",
        "\n",
        "    # --- 1. Label Encoding ---\n",
        "    # Create a label encoder object\n",
        "    label_encoder = LabelEncoder()\n",
        "\n",
        "    # Fit the encoder on the unique sentiment labels and transform the column\n",
        "    df['label'] = label_encoder.fit_transform(df[label_column])\n",
        "\n",
        "    # Store the mapping from labels to IDs and vice-versa (useful later)\n",
        "    label2id = {label: id for id, label in enumerate(label_encoder.classes_)}\n",
        "    id2label = {id: label for label, id in label2id.items()}\n",
        "\n",
        "    print(\"--- Label Encoding ---\")\n",
        "    print(\"Original Labels:\", label_encoder.classes_)\n",
        "    print(\"Encoded Labels (sample):\", df['label'].head().tolist())\n",
        "    print(\"Label to ID mapping:\", label2id)\n",
        "    print(\"ID to Label mapping:\", id2label)\n",
        "\n",
        "    # Keep only the text and the new numerical label\n",
        "    df_processed = df[[text_column, 'label']].rename(columns={text_column: 'text'}) # Rename to 'text' and 'label' for consistency\n",
        "\n",
        "    # --- 2. Split Data into Training and Testing Sets ---\n",
        "    # Stratify ensures that the proportion of labels is roughly the same in train and test sets\n",
        "    train_df, test_df = train_test_split(\n",
        "        df_processed,\n",
        "        test_size=0.2,        # Use 20% of the data for testing\n",
        "        random_state=42,      # For reproducible results\n",
        "        stratify=df_processed['label'] # Important for imbalanced datasets\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- Data Splitting ---\")\n",
        "    print(\"Total examples:\", len(df_processed))\n",
        "    print(\"Training examples:\", len(train_df))\n",
        "    print(\"Testing examples:\", len(test_df))\n",
        "    print(\"Training label distribution:\\n\", train_df['label'].value_counts(normalize=True).sort_index())\n",
        "    print(\"Testing label distribution:\\n\", test_df['label'].value_counts(normalize=True).sort_index())\n",
        "\n",
        "\n",
        "    # --- 3. Convert pandas DataFrames to Hugging Face Datasets ---\n",
        "    train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
        "    test_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
        "\n",
        "    # Combine them into a DatasetDict (standard practice for Hugging Face)\n",
        "    raw_datasets = DatasetDict({\n",
        "        'train': train_dataset,\n",
        "        'test': test_dataset\n",
        "    })\n",
        "\n",
        "    print(\"\\n--- Hugging Face Dataset Conversion ---\")\n",
        "    print(raw_datasets)\n",
        "\n",
        "    # Display a sample from the training set in the new format\n",
        "    print(\"\\nSample from training dataset:\")\n",
        "    print(raw_datasets['train'][0])"
      ],
      "metadata": {
        "id": "vOSTI9ZgfqXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# --- Choose the model checkpoint ---\n",
        "# Using DistilBERT - it's faster and often performs well for classification.\n",
        "# Other options: 'bert-base-uncased', 'ProsusAI/finbert' (if you want a finance-specific model)\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "\n",
        "# --- Load the tokenizer ---\n",
        "# AutoTokenizer automatically selects the correct tokenizer class based on the checkpoint\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "    print(f\"Tokenizer for '{model_checkpoint}' loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading tokenizer: {e}\")\n",
        "    # Handle error, maybe stop execution or try a different checkpoint\n",
        "    tokenizer = None\n",
        "\n",
        "if tokenizer is not None:\n",
        "    # --- Define the tokenization function ---\n",
        "    def tokenize_function(examples):\n",
        "        # Tokenize the 'text' batch.\n",
        "        # truncation=True ensures that inputs longer than the model's max length are cut off.\n",
        "        # padding=True pads shorter sequences to the max length in the batch (or overall if max_length specified).\n",
        "        return tokenizer(examples[\"text\"], truncation=True, padding=True)\n",
        "\n",
        "    # --- Apply the tokenization function to the datasets ---\n",
        "    # We use batched=True for faster processing.\n",
        "    # The map function applies our tokenize_function to batches of examples.\n",
        "    print(\"\\nTokenizing datasets...\")\n",
        "    tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "    print(\"Tokenization complete.\")\n",
        "\n",
        "    # --- Remove the original text column ---\n",
        "    # The model doesn't need the raw text after tokenization.\n",
        "    # It needs input_ids, attention_mask, and labels.\n",
        "    tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
        "\n",
        "    # --- Rename label column to match expected Hugging Face Trainer format ---\n",
        "    # The Trainer expects the label column to be named 'labels'. We already named it 'label'.\n",
        "    # Let's rename it just to be safe and standard.\n",
        "    tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
        "\n",
        "    # --- Set the format to PyTorch tensors ---\n",
        "    # This prepares the dataset for input into a PyTorch model.\n",
        "    tokenized_datasets.set_format(\"torch\")\n",
        "\n",
        "    print(\"\\n--- Post-Tokenization Processing ---\")\n",
        "    print(\"Columns removed: ['text']\")\n",
        "    print(\"Column 'label' renamed to 'labels'\")\n",
        "    print(\"Dataset format set to 'torch'\")\n",
        "\n",
        "    print(\"\\n--- Final Tokenized Dataset Structure ---\")\n",
        "    print(tokenized_datasets)\n",
        "\n",
        "    # Check an example from the tokenized training set\n",
        "    print(\"\\nSample from tokenized training dataset:\")\n",
        "    print(tokenized_datasets['train'][0])\n",
        "    # Notice the new columns: 'input_ids', 'attention_mask', and 'labels'"
      ],
      "metadata": {
        "id": "VRQrXkpUf_zC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "# --- Get the number of labels from our label mapping ---\n",
        "num_labels = len(label2id) # This should be 3 in our case (neutral, positive, negative/other)\n",
        "print(f\"Number of labels: {num_labels}\")\n",
        "\n",
        "# --- Load the model ---\n",
        "# AutoModelForSequenceClassification automatically adds a classification head\n",
        "# suited for the number of labels we specify.\n",
        "try:\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_checkpoint,                     # e.g., \"distilbert-base-uncased\"\n",
        "        num_labels=num_labels,                # The number of output classes\n",
        "        id2label=id2label,                    # Pass mapping for nicer output labels\n",
        "        label2id=label2id                     # Pass mapping\n",
        "    )\n",
        "    print(f\"Model '{model_checkpoint}' loaded successfully for sequence classification.\")\n",
        "    # You can optionally print model structure:\n",
        "    # print(model)\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    # Handle error\n",
        "    model = None\n",
        "\n",
        "# --- Check if GPU is available and move model to GPU ---\n",
        "import torch\n",
        "\n",
        "if model is not None:\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    print(f\"Model moved to device: {device}\")\n",
        "\n",
        "    # Verify the model configuration (optional)\n",
        "    print(\"\\n--- Model Configuration ---\")\n",
        "    print(f\"Expected number of labels: {model.config.num_labels}\")\n",
        "    print(f\"Label mapping (id2label): {model.config.id2label}\")"
      ],
      "metadata": {
        "id": "u8seajbuhJrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure necessary libraries are installed (evaluate might need separate install)\n",
        "!pip install evaluate scikit-learn -U # Add scikit-learn just in case\n",
        "\n",
        "import evaluate # Hugging Face's library for evaluation metrics\n",
        "import numpy as np\n",
        "\n",
        "# --- Load the metrics ---\n",
        "# We'll use accuracy and F1-score (which includes precision and recall implicitly for micro/macro)\n",
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "# You could also load precision and recall separately if needed:\n",
        "# precision_metric = evaluate.load(\"precision\")\n",
        "# recall_metric = evaluate.load(\"recall\")\n",
        "\n",
        "\n",
        "# --- Define the compute_metrics function ---\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Computes accuracy and F1 score for evaluation predictions.\"\"\"\n",
        "    logits, labels = eval_pred # Unpack the predictions tuple\n",
        "\n",
        "    # Get the predicted class by finding the index with the highest logit score\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "    # Calculate F1 score - use 'weighted' average for imbalanced datasets\n",
        "    # 'macro' treats each class equally, 'micro' aggregates globally\n",
        "    f1_weighted = f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
        "    f1_macro = f1_metric.compute(predictions=predictions, references=labels, average=\"macro\")\n",
        "\n",
        "\n",
        "    # Return metrics as a dictionary\n",
        "    return {\n",
        "        \"accuracy\": accuracy[\"accuracy\"],\n",
        "        \"f1_weighted\": f1_weighted[\"f1\"],\n",
        "        \"f1_macro\": f1_macro[\"f1\"]\n",
        "        # Add precision/recall if you loaded them\n",
        "        # \"precision\": precision_metric.compute(predictions=predictions, references=labels, average=\"weighted\")[\"precision\"],\n",
        "        # \"recall\": recall_metric.compute(predictions=predictions, references=labels, average=\"weighted\")[\"recall\"],\n",
        "    }\n",
        "\n",
        "# --- Test the function with dummy data (optional) ---\n",
        "print(\"Testing compute_metrics function with dummy data:\")\n",
        "dummy_logits = np.array([[0.1, 0.9, 0.0], [-0.1, 0.1, 1.0], [0.8, 0.1, 0.1]])\n",
        "dummy_labels = np.array([1, 2, 0]) # Corresponding true labels\n",
        "dummy_eval_pred = (dummy_logits, dummy_labels)\n",
        "test_metrics = compute_metrics(dummy_eval_pred)\n",
        "print(test_metrics)\n",
        "# Expected output: accuracy=1.0, f1_weighted=1.0, f1_macro=1.0 as predictions match labels perfectly here."
      ],
      "metadata": {
        "id": "YkdEHk3ahbyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# --- Define Training Arguments ---\n",
        "\n",
        "# Choose a name for your model repository (if pushing to Hub) or local output folder\n",
        "repo_name = \"finetuned-financial-sentiment\" # You can change this name\n",
        "\n",
        "# Check if running in Colab (for suggested output directory)\n",
        "try:\n",
        "    import google.colab\n",
        "    output_dir = f\"./{repo_name}\" # Save output within the Colab environment's file system\n",
        "    print(f\"Running in Colab. Output directory set to: {output_dir}\")\n",
        "except ImportError:\n",
        "    output_dir = f\"./{repo_name}\" # Save in current directory if not in Colab\n",
        "    print(f\"Not running in Colab. Output directory set to: {output_dir}\")\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,                  # Directory to save model checkpoints and logs\n",
        "    num_train_epochs=3,                     # Total number of training epochs (3 is a common starting point)\n",
        "    per_device_train_batch_size=16,         # Batch size per GPU for training (adjust based on GPU memory)\n",
        "    per_device_eval_batch_size=32,          # Batch size per GPU for evaluation\n",
        "    learning_rate=2e-5,                     # Learning rate (common default for fine-tuning transformers)\n",
        "    weight_decay=0.01,                      # Strength of weight decay regularization\n",
        "    evaluation_strategy=\"epoch\",            # Evaluate model performance at the end of each epoch\n",
        "    save_strategy=\"epoch\",                  # Save model checkpoint at the end of each epoch\n",
        "    logging_strategy=\"epoch\",               # Log metrics at the end of each epoch\n",
        "    load_best_model_at_end=True,            # Load the best checkpoint (based on metric) when training finishes\n",
        "    metric_for_best_model=\"f1_weighted\",    # Use weighted F1 score to determine the best model\n",
        "    greater_is_better=True,                 # Higher F1 score is better\n",
        "    push_to_hub=False,                      # Set to True if you want to upload to Hugging Face Hub (requires login)\n",
        "    report_to=\"tensorboard\",                # Log to tensorboard (can view graphs in Colab)\n",
        "    # Optional: If you have issues with CUDA memory, try gradient accumulation\n",
        "    # gradient_accumulation_steps=2,\n",
        ")\n",
        "\n",
        "# --- Initialize the Trainer ---\n",
        "\n",
        "# Make sure all components exist before initializing\n",
        "if 'model' in locals() and model is not None and \\\n",
        "   'tokenized_datasets' in locals() and \\\n",
        "   'tokenizer' in locals() and tokenizer is not None and \\\n",
        "   'compute_metrics' in locals():\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,                            # The instantiated Transformers model to be trained\n",
        "        args=training_args,                     # Training arguments, defined above\n",
        "        train_dataset=tokenized_datasets[\"train\"], # Training dataset\n",
        "        eval_dataset=tokenized_datasets[\"test\"],  # Evaluation dataset\n",
        "        tokenizer=tokenizer,                    # Tokenizer (needed for padding consistency)\n",
        "        compute_metrics=compute_metrics,        # Function to compute metrics during evaluation\n",
        "    )\n",
        "    print(\"Trainer initialized successfully.\")\n",
        "else:\n",
        "    print(\"Error: One or more components (model, datasets, tokenizer, compute_metrics) not found.\")\n",
        "    trainer = None # Set trainer to None if initialization fails\n",
        "\n",
        "# (Optional) If you want to push to Hugging Face Hub (requires push_to_hub=True above)\n",
        "# You'll need to log in. Uncomment and run the following lines in a separate cell:\n",
        "# from huggingface_hub import notebook_login\n",
        "# notebook_login()"
      ],
      "metadata": {
        "id": "dsjJxc65hqYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Start Training ---\n",
        "if trainer is not None:\n",
        "    print(\"Starting training...\")\n",
        "    try:\n",
        "        train_result = trainer.train()\n",
        "        print(\"Training finished.\")\n",
        "\n",
        "        # --- Optional: Save metrics and state ---\n",
        "        metrics = train_result.metrics\n",
        "        trainer.log_metrics(\"train\", metrics)\n",
        "        trainer.save_metrics(\"train\", metrics)\n",
        "        trainer.save_state()\n",
        "        print(\"Training metrics saved.\")\n",
        "\n",
        "        # The trainer automatically saves the best model if load_best_model_at_end=True\n",
        "        # You can also save it manually if needed:\n",
        "        # trainer.save_model(f\"{output_dir}/final_model\")\n",
        "        # print(f\"Final model saved to {output_dir}/final_model\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during training: {e}\")\n",
        "else:\n",
        "    print(\"Trainer was not initialized. Cannot start training.\")"
      ],
      "metadata": {
        "id": "u8CP_7KAh5QJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Evaluate the final loaded model on the test set ---\n",
        "if trainer is not None:\n",
        "    print(\"\\nEvaluating the final model on the test set...\")\n",
        "    eval_results = trainer.evaluate() # Uses the test_dataset specified during Trainer init\n",
        "\n",
        "    print(\"\\n--- Final Evaluation Results ---\")\n",
        "    print(eval_results)\n",
        "\n",
        "    # You can also save these final metrics\n",
        "    trainer.log_metrics(\"eval\", eval_results)\n",
        "    trainer.save_metrics(\"eval\", eval_results)\n",
        "else:\n",
        "    print(\"Trainer not available for evaluation.\")"
      ],
      "metadata": {
        "id": "IWAx9hdqiHYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import torch # Ensure torch is imported\n",
        "\n",
        "# --- Create a text classification pipeline ---\n",
        "\n",
        "# Use the 'trainer.model' which is the best model loaded after training\n",
        "# Or, if you saved the model, you could load it from the output directory:\n",
        "# model_path = f\"{output_dir}/best_model_or_last_checkpoint\" # Adjust path as needed\n",
        "# classifier = pipeline(\"text-classification\", model=model_path, tokenizer=tokenizer, device=device)\n",
        "\n",
        "if trainer is not None and tokenizer is not None:\n",
        "    # Ensure the model is on the correct device (it should be, but double-check)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    trainer.model.to(device)\n",
        "\n",
        "    # Create pipeline using the model and tokenizer from the trainer\n",
        "    classifier = pipeline(\n",
        "        \"text-classification\",        # Task type\n",
        "        model=trainer.model,          # The fine-tuned model\n",
        "        tokenizer=tokenizer,        # The tokenizer used during training\n",
        "        device=0 if device.type == 'cuda' else -1 # Specify GPU (0) or CPU (-1) for pipeline\n",
        "    )\n",
        "    print(f\"Classifier pipeline created on device: {device}\")\n",
        "\n",
        "    # --- Example Sentences ---\n",
        "    new_sentences = [\n",
        "        \"Revenue increased significantly compared to the last quarter.\",\n",
        "        \"The company announced unexpected losses and job cuts.\",\n",
        "        \"Stock prices remained relatively stable despite market fluctuations.\",\n",
        "        \"Analysts are neutral about the future prospects of the tech sector.\",\n",
        "        \"Earnings per share beat expectations, causing a surge in stock value.\",\n",
        "        \"There are concerns about the upcoming regulatory changes.\"\n",
        "    ]\n",
        "\n",
        "    # --- Get Predictions ---\n",
        "    print(\"\\n--- Predicting Sentiment for New Sentences ---\")\n",
        "    predictions = classifier(new_sentences)\n",
        "\n",
        "    # Print predictions\n",
        "    for sentence, prediction in zip(new_sentences, predictions):\n",
        "        print(f\"Sentence: {sentence}\")\n",
        "        print(f\"Predicted Label: {prediction['label']} (ID: {label2id[prediction['label']]})\") # Use label2id mapping\n",
        "        print(f\"Confidence Score: {prediction['score']:.4f}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "else:\n",
        "    print(\"Cannot create pipeline: Trainer or Tokenizer not available.\")"
      ],
      "metadata": {
        "id": "b0rpAh1Ti6zV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p2sNpAnli_kV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}